{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy.random as random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harryxue/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:1269: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('/Users/harryxue/Documents/Siamese_FirstTrack_128x128.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d_1_1/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv2d_2_1/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv2d_4_1/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'conv2d_3_1/add:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'dense_1_1/add:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURE_DIR = '../data/features/mel_spec_first/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use 128x128 samples (~3 second samples) to speed up training\n",
    "# TODO: Use full 30 second samples\n",
    "INPUT_DIM = (128, 128, 1)\n",
    "# Number of frames to sample\n",
    "N_FRAMES = INPUT_DIM[1]\n",
    "\n",
    "MODEL_SAVE_NAME = 'Siamese_FirstTrack_{}x{}.hdf5'.format(INPUT_DIM[0], INPUT_DIM[1])\n",
    "\n",
    "# Training configurations\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harryxue/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:36: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/harryxue/anaconda2/lib/python2.7/site-packages/keras/legacy/layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Users/harryxue/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68311873"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from https://sorenbouma.github.io/blog/oneshot/\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = random.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=random.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "input_shape = INPUT_DIM\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "# Build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n",
    "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(7,7),activation='relu',\n",
    "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "\n",
    "# Encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "# Merge two encoded inputs with the l1 distance between them\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
    "siamese_net = Model(input=[left_input,right_input],output=prediction)\n",
    "#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n",
    "\n",
    "optimizer = Adam(0.00006)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "siamese_net.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "influences = pd.read_csv('../data/allmusic/influences_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create lookup dictionary mapping file id to artist id\n",
    "file2id = {}\n",
    "\n",
    "for filename in os.listdir(FEATURE_DIR):\n",
    "    file2id[filename] = int(filename.split('.npy')[0]) \n",
    "\n",
    "id2file = {id:filename for (filename, id) in file2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Filter out for influence relationships where we have features for both influencer\n",
    "# # and follower\n",
    "# ids_with_features = np.unique(id2file.keys())\n",
    "# influences = influences[influences['influencer_id'].isin(ids_with_features) & influences['follower_id'].isin(ids_with_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Positive examples\n",
    "# positive_inf = influences[['influencer_id', 'follower_id']].values\n",
    "# # Convert to set for comparison when generating negative examples\n",
    "# positive_inf_set = set([tuple(l) for l in positive_inf.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate same number of negative examples of influence relationships as positive\n",
    "# negative_inf = []\n",
    "\n",
    "# while len(negative_inf) < len(positive_inf):\n",
    "#     id1 = random.choice(ids_with_features)\n",
    "#     id2 = random.choice(ids_with_features)\n",
    "    \n",
    "#     if id1 != id2 and (id1, id2) not in positive_inf_set:\n",
    "#         negative_inf.append([id1, id2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 80-20 Train-val split for influence relationships\n",
    "# pos_train_rel, pos_val_rel, neg_train_rel, neg_val_rel = train_test_split(positive_inf, np.array(negative_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('relationships/pos_train_rel.npy', pos_train_rel)\n",
    "# np.save('relationships/pos_val_rel', pos_val_rel)\n",
    "# np.save('relationships/neg_train_rel', neg_train_rel)\n",
    "# np.save('relationships/neg_val_rel', neg_val_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load positive and negative influence pairs\n",
    "pos_train_rel, pos_val_rel, neg_train_rel, neg_val_rel = np.load('relationships/pos_train_rel.npy'), np.load('relationships/pos_val_rel.npy'), np.load('relationships/neg_train_rel.npy'), np.load('relationships/neg_val_rel.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_frames(melspec, n_frames):\n",
    "    \"\"\"Sample n_frames (contiguous) from a melspec representation\"\"\"\n",
    "    total_frames = melspec.shape[1]\n",
    "    sample_range = range(0, total_frames - n_frames + 1)\n",
    "    sample_index = random.choice(sample_range)\n",
    "    \n",
    "    return melspec[:, sample_index:sample_index + n_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(pos_ex, neg_ex, batch_size=BATCH_SIZE, n_frames=N_FRAMES):\n",
    "    \"\"\"\n",
    "        Infinite generator for batches of data containing equal numbers of\n",
    "        positive and negative pairs of mel-spec sample pairs with influence\n",
    "        relationships present and not present\n",
    "    \"\"\"\n",
    "    # Create binary labels for examples\n",
    "    labels = np.concatenate((np.ones(len(pos_ex)), np.zeros(len(neg_ex))))\n",
    "    all_ex = list(zip(np.concatenate((pos_ex, neg_ex)), labels))\n",
    "    examples, labels = zip(*all_ex)\n",
    "    \n",
    "    batch_first, batch_second, batch_Y = [], [], []\n",
    "    \n",
    "    while True:\n",
    "        random.shuffle(all_ex)\n",
    "        examples, labels = zip(*all_ex)\n",
    "            \n",
    "        for example, label in zip(examples, labels):\n",
    "            # Extract sample of size N_FRAMES from mel_spec representation\n",
    "            # for each example in pair\n",
    "            pair_first, pair_second = [sample_frames(np.load(FEATURE_DIR + id2file[id]), n_frames) for id in example]\n",
    "            batch_first.append(pair_first[:, :,np.newaxis])\n",
    "            batch_second.append(pair_second[:, :, np.newaxis])\n",
    "            batch_Y.append(label)\n",
    "                \n",
    "            if len(batch_first) == batch_size:\n",
    "                yield [np.array(batch_first), np.array(batch_second)], np.array(batch_Y)\n",
    "                batch_first, batch_second, batch_Y = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint = ModelCheckpoint(MODEL_SAVE_NAME, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = siamese_net.fit_generator(generator = generator(pos_train_rel, neg_train_rel),\n",
    "                    steps_per_epoch = len(pos_train_rel)//BATCH_SIZE,\n",
    "                    validation_data = generator(pos_val_rel, neg_val_rel),\n",
    "                    validation_steps = len(pos_val_rel)//BATCH_SIZE,\n",
    "                    callbacks=[early_stopping, checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
