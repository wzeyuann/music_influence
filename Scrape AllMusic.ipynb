{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "from random import choice\n",
    "import sys\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Bjork's page as root (to ensure we're dealing with unicode correctly)\n",
    "BASE_URL = 'https://www.allmusic.com/artist/bj%C3%B6rk-mn0000769444'\n",
    "\n",
    "# Path to files\n",
    "artist_file = \"data/allmusic/artists.txt\"\n",
    "influence_file = \"data/allmusic/influences.txt\"\n",
    "\n",
    "# List of headers to spoof\n",
    "headers = [\"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "          \"Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0\",\n",
    "          \"Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16\",\n",
    "          \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\",\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artist_out = open(artist_file, 'w')\n",
    "influence_out = open(influence_file, 'w')\n",
    "artist_writer = csv.writer(artist_out)\n",
    "influence_writer = csv.writer(influence_out)\n",
    "\n",
    "# Explored set\n",
    "explored = set()\n",
    "\n",
    "# Place root page onto the queue\n",
    "url_queue = deque([BASE_URL])\n",
    "\n",
    "# Run Breadth First Search on musician pages\n",
    "while len(url_queue) != 0:\n",
    "    print \"Size of queue:\", len(url_queue)\n",
    "    print \"Artists explored:\", len(explored)\n",
    "\n",
    "    current_base_url = url_queue.popleft()\n",
    "    current_url = current_base_url + '/related'\n",
    "\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "\n",
    "        # Spoof a random header\n",
    "        session.headers.update({'User-Agent': choice(headers)})\n",
    "        page = session.get(current_url)\n",
    "\n",
    "        # Sleep for random number of seconds\n",
    "        sleep(choice([4,5]))\n",
    "        soup = BeautifulSoup(page.content.decode('utf-8'), 'html.parser')\n",
    "\n",
    "        # Extract Name\n",
    "        try:\n",
    "            name = soup.find_all('h1', class_='artist-name')[0].text.strip()\n",
    "        except:\n",
    "            print \"Name Extraction Failure\", current_url\n",
    "            name = \"NA\"\n",
    "\n",
    "        # Extract Active Period\n",
    "        try:\n",
    "            active = soup.find_all('div', class_='active-dates')[0].find_all('div')[0].text\n",
    "        except:\n",
    "            print \"Active Period Extraction Failure\", current_url\n",
    "            active = \"NA\"\n",
    "\n",
    "        # Extract the genres associated with the musician\n",
    "        try:\n",
    "            genre_tags = soup.find_all('div', class_='genre')[0].find_all('div')[0]\n",
    "            genres = ('|').join([genre.string for genre in genre_tags if not genre.string.isspace()])\n",
    "        except:\n",
    "            print \"Genre Extraction Failure\", current_url\n",
    "            genres = \"NA\"\n",
    "\n",
    "        # Extract the list of styles associated with the musician       \n",
    "        try:\n",
    "            style_tags = soup.find_all('div', class_='styles')[0].find_all('div')[0]\n",
    "            styles = ('|').join([style.string for style in style_tags if not style.string.isspace()])\n",
    "        except:\n",
    "            print \"Style Extraction Failure\", current_url\n",
    "            styles = \"NA\"\n",
    "\n",
    "        # Extract the musician's influencers (musicians who were influenced him)\n",
    "        try:\n",
    "            influencers_list = soup.find_all('section', attrs={'class':'related influencers'})[0]\\\n",
    "            .find_next('ul')\\\n",
    "            .find_all('li')\n",
    "\n",
    "            for influencer_item in influencers_list:\n",
    "                influencer_name = influencer_item.text.strip()\n",
    "                influencer_link = influencer_item.a.attrs['href']\n",
    "\n",
    "                influence_relationship = [influencer_name.encode('utf-8'), influencer_link.encode('utf-8'), name.encode('utf-8'), current_base_url.encode('utf-8')]\n",
    "                print influence_relationship\n",
    "                influence_writer.writerow(influence_relationship)\n",
    "\n",
    "                if influencer_link not in explored:\n",
    "                    explored.add(influencer_link)\n",
    "                    url_queue.append(influencer_link)\n",
    "        except:\n",
    "                print \"Influencers Extraction Failure\", current_url\n",
    "\n",
    "        # Extract the musician's followers (musicians who were influenced by him)\n",
    "        try:\n",
    "            followers_list = soup.find_all('section', attrs={'class':'related followers'})[0]\\\n",
    "            .find_next('ul')\\\n",
    "            .find_all('li')\n",
    "\n",
    "            for follower_item in followers_list:\n",
    "                follower_name = follower_item.text.strip()\n",
    "                follower_link = follower_item.a.attrs['href']\n",
    "\n",
    "                follower_relationship = [name.encode('utf-8'), current_base_url.encode('utf-8'), follower_name.encode('utf-8'), follower_link.encode('utf-8')]\n",
    "                print follower_relationship\n",
    "                influence_writer.writerow(follower_relationship)\n",
    "\n",
    "                if follower_link not in explored:\n",
    "                    explored.add(follower_link)\n",
    "                    url_queue.append(follower_link)\n",
    "        except:\n",
    "            print \"Followers Extraction Failure\", current_url\n",
    "\n",
    "        influence_out.flush()\n",
    "\n",
    "\n",
    "        # Write artist information\n",
    "        try:\n",
    "            artist_info = [name.encode('utf-8'), current_base_url.encode('utf-8'), active.encode('utf-8'), genres.encode('utf-8'), styles.encode('utf-8')]\n",
    "            print artist_info\n",
    "            artist_writer.writerow(artist_info)\n",
    "            artist_out.flush()\n",
    "        except:\n",
    "            print \"Failure writing artist information\", current_url\n",
    "\n",
    "    except:\n",
    "        # Sleep for just over an hour if blocked by server\n",
    "        print \"Request Failure: Sleeping for an hour...\"\n",
    "        sleep(60 * 61)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
